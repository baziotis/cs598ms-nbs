{
  "cells": [
    {
      "raw": "# !pip install -U vega_datasets notebook vega",
      "total-ns": 560612
    },
    {
      "raw": "import numpy as np\nimport pandas as pd\nimport os\n\nimport matplotlib.pyplot as plt\n%matplotlib inline\nfrom tqdm import tqdm_notebook\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.svm import NuSVR, SVR\nfrom sklearn.metrics import mean_absolute_error\npd.options.display.precision = 15\n\n# import lightgbm as lgb\n# import xgboost as xgb\nimport time\nimport datetime\n# from catboost import CatBoostRegressor\nfrom sklearn.preprocessing import LabelEncoder\n# from sklearn.model_selection import StratifiedKFold, KFold, RepeatedKFold, GroupKFold, GridSearchCV, train_test_split, TimeSeriesSplit\n# from sklearn import metrics\n# from sklearn import linear_model\nimport gc\nimport seaborn as sns\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n# import eli5\n# import shap\nfrom IPython.display import HTML\nimport json\n# import altair as alt\n\nimport networkx as nx\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n# alt.renderers.enable('notebook')\n\n%env JOBLIB_TEMP_FOLDER=/tmp",
      "total-ns": 5354081708
    },
    {
      "raw": "# There is the training code here, but deleted it because it was very long",
      "total-ns": 395215
    },
    {
      "raw": "folder_path = '../input/'\ntrain_identity = pd.read_csv(f'{folder_path}train_identity.csv')\ntrain_transaction = pd.read_csv(f'{folder_path}train_transaction.csv')\ntest_identity = pd.read_csv(f'{folder_path}test_identity.csv')\ntest_transaction = pd.read_csv(f'{folder_path}test_transaction.csv')\nsub = pd.read_csv(f'{folder_path}sample_submission.csv')\n# let's combine the data and work with the whole dataset\ntrain = pd.merge(train_transaction, train_identity, on='TransactionID', how='left')\ntest = pd.merge(test_transaction, test_identity, on='TransactionID', how='left')",
      "total-ns": 60281124271
    },
    {
      "raw": "print(f'Train dataset has {train.shape[0]} rows and {train.shape[1]} columns.')\nprint(f'Test dataset has {test.shape[0]} rows and {test.shape[1]} columns.')",
      "total-ns": 11608797
    },
    {
      "raw": "train_transaction.head()",
      "total-ns": 6318982
    },
    {
      "raw": "train_identity.head()",
      "total-ns": 1086954
    },
    {
      "raw": "del train_identity, train_transaction, test_identity, test_transaction",
      "total-ns": 461275378
    },
    {
      "raw": "print(f'There are {train.isnull().any().sum()} columns in train dataset with missing values.')",
      "total-ns": 4821129030
    },
    {
      "raw": "one_value_cols = [col for col in train.columns if train[col].nunique() <= 1]\none_value_cols_test = [col for col in test.columns if test[col].nunique() <= 1]\none_value_cols == one_value_cols_test",
      "total-ns": 6738489989
    },
    {
      "raw": "print(f'There are {len(one_value_cols)} columns in train dataset with one unique value.')\nprint(f'There are {len(one_value_cols_test)} columns in test dataset with one unique value.')",
      "total-ns": 575736
    },
    {
      "raw": "# plt.hist(train['id_01'], bins=77);\n# plt.title('Distribution of id_01 variable');",
      "total-ns": 136482
    },
    {
      "raw": "train['id_03'].value_counts(dropna=False, normalize=True).head()",
      "total-ns": 9861603
    },
    {
      "raw": "train['id_11'].value_counts(dropna=False, normalize=True).head()",
      "total-ns": 7095523
    },
    {
      "raw": "# plt.hist(train['id_07']);\n# plt.title('Distribution of id_07 variable');",
      "total-ns": 183868
    },
    {
      "raw": "charts = {}\nfor i in ['id_12', 'id_15', 'id_16', 'id_28', 'id_29', 'id_30', 'id_31', 'id_32', 'id_33', 'id_34', 'id_35', 'id_36', 'id_37', 'id_38']:\n    feature_count = train[i].value_counts(dropna=False).reset_index().rename(columns={i: 'count', 'index': i})\n#     chart = alt.Chart(feature_count).mark_bar().encode(\n#                 y=alt.Y(f\"{i}:N\", axis=alt.Axis(title=i)),\n#                 x=alt.X('count:Q', axis=alt.Axis(title='Count')),\n#                 tooltip=[i, 'count']\n#             ).properties(title=f\"Counts of {i}\", width=400)\n#     charts[i] = chart                         \n    \n# render((charts['id_12'] | charts['id_15'] | charts['id_16']) & (charts['id_28'] | charts['id_29'] | charts['id_32']) & (charts['id_34'] | charts['id_35'] | charts['id_36']) & (charts['id_37'] | charts['id_38']))",
      "total-ns": 169329500
    },
    {
      "raw": "charts = {}\nfor i in ['id_30', 'id_31', 'id_33', 'DeviceType', 'DeviceInfo']:\n    feature_count = train[i].value_counts(dropna=False)[:40].reset_index().rename(columns={i: 'count', 'index': i})\n#     chart = alt.Chart(feature_count).mark_bar().encode(\n#                 x=alt.X(f\"{i}:N\", axis=alt.Axis(title=i)),\n#                 y=alt.Y('count:Q', axis=alt.Axis(title='Count')),\n#                 tooltip=[i, 'count']\n#             ).properties(title=f\"Counts of {i}\", width=800)\n#     charts[i] = chart\n    \n# render(charts['id_30'] & charts['id_31'] & charts['id_33'] & charts['DeviceType'] & charts['DeviceInfo'])",
      "total-ns": 62793190
    },
    {
      "raw": "# plt.hist(train['TransactionDT'], label='train');\n# plt.hist(test['TransactionDT'], label='test');\n# plt.legend();\n# plt.title('Distribution of transactiond dates');",
      "total-ns": 191825
    },
    {
      "raw": "charts = {}\nfor i in ['ProductCD', 'card4', 'card6', 'M4', 'M1', 'M2', 'M3', 'M5', 'M6', 'M7', 'M8', 'M9']:\n    feature_count = train[i].value_counts(dropna=False).reset_index().rename(columns={i: 'count', 'index': i})\n#     chart = alt.Chart(feature_count).mark_bar().encode(\n#                 y=alt.Y(f\"{i}:N\", axis=alt.Axis(title=i)),\n#                 x=alt.X('count:Q', axis=alt.Axis(title='Count')),\n#                 tooltip=[i, 'count']\n#             ).properties(title=f\"Counts of {i}\", width=400)\n#     charts[i] = chart                         \n    \n# render((charts['ProductCD'] | charts['card4']) & (charts['card6'] | charts['M4']) & (charts['card6'] | charts['M4']) & (charts['M1'] | charts['M2']) & (charts['M3'] | charts['M5']) & (charts['M6'] | charts['M7']) & (charts['M8'] | charts['M9']))",
      "total-ns": 151494738
    },
    {
      "raw": "charts = {}\nfor i in ['P_emaildomain', 'R_emaildomain', 'card1', 'card2', 'card3',  'card5', 'addr1', 'addr2']:\n    feature_count = train[i].value_counts(dropna=False).reset_index()[:40].rename(columns={i: 'count', 'index': i})\n#     chart = alt.Chart(feature_count).mark_bar().encode(\n#                 x=alt.X(f\"{i}:N\", axis=alt.Axis(title=i)),\n#                 y=alt.Y('count:Q', axis=alt.Axis(title='Count')),\n#                 tooltip=[i, 'count']\n#             ).properties(title=f\"Counts of {i}\", width=600)\n#     charts[i] = chart\n    \n# render((charts['P_emaildomain'] | charts['R_emaildomain']) & (charts['card1'] | charts['card2']) & (charts['card3'] | charts['card5']) & (charts['addr1'] | charts['addr2']))",
      "total-ns": 79978830
    },
    {
      "raw": "train['TransactionAmt_to_mean_card1'] = train['TransactionAmt'] / train.groupby(['card1'])['TransactionAmt'].transform('mean')\ntrain['TransactionAmt_to_mean_card4'] = train['TransactionAmt'] / train.groupby(['card4'])['TransactionAmt'].transform('mean')\ntrain['TransactionAmt_to_std_card1'] = train['TransactionAmt'] / train.groupby(['card1'])['TransactionAmt'].transform('std')\ntrain['TransactionAmt_to_std_card4'] = train['TransactionAmt'] / train.groupby(['card4'])['TransactionAmt'].transform('std')\n\n# test['TransactionAmt_to_mean_card1'] = test['TransactionAmt'] / test.groupby(['card1'])['TransactionAmt'].transform('mean')\n# test['TransactionAmt_to_mean_card4'] = test['TransactionAmt'] / test.groupby(['card4'])['TransactionAmt'].transform('mean')\n# test['TransactionAmt_to_std_card1'] = test['TransactionAmt'] / test.groupby(['card1'])['TransactionAmt'].transform('std')\n# test['TransactionAmt_to_std_card4'] = test['TransactionAmt'] / test.groupby(['card4'])['TransactionAmt'].transform('std')\n\ntrain['id_02_to_mean_card1'] = train['id_02'] / train.groupby(['card1'])['id_02'].transform('mean')\ntrain['id_02_to_mean_card4'] = train['id_02'] / train.groupby(['card4'])['id_02'].transform('mean')\ntrain['id_02_to_std_card1'] = train['id_02'] / train.groupby(['card1'])['id_02'].transform('std')\ntrain['id_02_to_std_card4'] = train['id_02'] / train.groupby(['card4'])['id_02'].transform('std')\n\n# test['id_02_to_mean_card1'] = test['id_02'] / test.groupby(['card1'])['id_02'].transform('mean')\n# test['id_02_to_mean_card4'] = test['id_02'] / test.groupby(['card4'])['id_02'].transform('mean')\n# test['id_02_to_std_card1'] = test['id_02'] / test.groupby(['card1'])['id_02'].transform('std')\n# test['id_02_to_std_card4'] = test['id_02'] / test.groupby(['card4'])['id_02'].transform('std')\n\ntrain['D15_to_mean_card1'] = train['D15'] / train.groupby(['card1'])['D15'].transform('mean')\ntrain['D15_to_mean_card4'] = train['D15'] / train.groupby(['card4'])['D15'].transform('mean')\ntrain['D15_to_std_card1'] = train['D15'] / train.groupby(['card1'])['D15'].transform('std')\ntrain['D15_to_std_card4'] = train['D15'] / train.groupby(['card4'])['D15'].transform('std')\n\n# test['D15_to_mean_card1'] = test['D15'] / test.groupby(['card1'])['D15'].transform('mean')\n# test['D15_to_mean_card4'] = test['D15'] / test.groupby(['card4'])['D15'].transform('mean')\n# test['D15_to_std_card1'] = test['D15'] / test.groupby(['card1'])['D15'].transform('std')\n# test['D15_to_std_card4'] = test['D15'] / test.groupby(['card4'])['D15'].transform('std')\n\ntrain['D15_to_mean_addr1'] = train['D15'] / train.groupby(['addr1'])['D15'].transform('mean')\ntrain['D15_to_mean_addr2'] = train['D15'] / train.groupby(['addr2'])['D15'].transform('mean')\ntrain['D15_to_std_addr1'] = train['D15'] / train.groupby(['addr1'])['D15'].transform('std')\ntrain['D15_to_std_addr2'] = train['D15'] / train.groupby(['addr2'])['D15'].transform('std')\n\n# test['D15_to_mean_addr1'] = test['D15'] / test.groupby(['addr1'])['D15'].transform('mean')\n# test['D15_to_mean_addr2'] = test['D15'] / test.groupby(['addr2'])['D15'].transform('mean')\n# test['D15_to_std_addr1'] = test['D15'] / test.groupby(['addr1'])['D15'].transform('std')\n# test['D15_to_std_addr2'] = test['D15'] / test.groupby(['addr2'])['D15'].transform('std')",
      "total-ns": 485197418
    },
    {
      "raw": "train[['P_emaildomain_1', 'P_emaildomain_2', 'P_emaildomain_3']] = train['P_emaildomain'].str.split('.', expand=True)\ntrain[['R_emaildomain_1', 'R_emaildomain_2', 'R_emaildomain_3']] = train['R_emaildomain'].str.split('.', expand=True)\n# test[['P_emaildomain_1', 'P_emaildomain_2', 'P_emaildomain_3']] = test['P_emaildomain'].str.split('.', expand=True)\n# test[['R_emaildomain_1', 'R_emaildomain_2', 'R_emaildomain_3']] = test['R_emaildomain'].str.split('.', expand=True)",
      "total-ns": 2839926349
    },
    {
      "raw": "many_null_cols = [col for col in train.columns if train[col].isnull().sum() / train.shape[0] > 0.9]\n# many_null_cols_test = [col for col in test.columns if test[col].isnull().sum() / test.shape[0] > 0.9]",
      "total-ns": 941321382
    },
    {
      "raw": "big_top_value_cols = [col for col in train.columns if train[col].value_counts(dropna=False, normalize=True).values[0] > 0.9]\n# big_top_value_cols_test = [col for col in test.columns if test[col].value_counts(dropna=False, normalize=True).values[0] > 0.9]",
      "total-ns": 3511154532
    },
    {
      "raw": "cols_to_drop = list(set(many_null_cols + big_top_value_cols + one_value_cols))\ncols_to_drop.remove('isFraud')\nlen(cols_to_drop)",
      "total-ns": 502406
    },
    {
      "raw": "train = train.drop(cols_to_drop, axis=1)\n# test = test.drop(cols_to_drop, axis=1)",
      "total-ns": 1439631051
    },
    {
      "raw": "cat_cols = ['id_12', 'id_13', 'id_14', 'id_15', 'id_16', 'id_17', 'id_18', 'id_19', 'id_20', 'id_21', 'id_22', 'id_23', 'id_24', 'id_25', 'id_26', 'id_27', 'id_28', 'id_29',\n            'id_30', 'id_31', 'id_32', 'id_33', 'id_34', 'id_35', 'id_36', 'id_37', 'id_38', 'DeviceType', 'DeviceInfo', 'ProductCD', 'card4', 'card6', 'M4','P_emaildomain',\n            'R_emaildomain', 'card1', 'card2', 'card3',  'card5', 'addr1', 'addr2', 'M1', 'M2', 'M3', 'M5', 'M6', 'M7', 'M8', 'M9',\n            'P_emaildomain_1', 'P_emaildomain_2', 'P_emaildomain_3', 'R_emaildomain_1', 'R_emaildomain_2', 'R_emaildomain_3']\nfor col in cat_cols:\n    if col in train.columns:\n        le = LabelEncoder()\n        le.fit(list(train[col].astype(str).values))\n        train[col] = le.transform(list(train[col].astype(str).values))\n#         test[col] = le.transform(list(test[col].astype(str).values))   ",
      "total-ns": 28852701663
    },
    {
      "raw": "X = train.sort_values('TransactionDT').drop(['isFraud', 'TransactionDT', 'TransactionID'], axis=1)\ny = train.sort_values('TransactionDT')['isFraud']\n#X_test = test.sort_values('TransactionDT').drop(['TransactionDT', 'TransactionID'], axis=1)\n# X_test = test.drop(['TransactionDT', 'TransactionID'], axis=1)\ndel train\n# test = test[[\"TransactionDT\", 'TransactionID']]",
      "total-ns": 4664763434
    },
    {
      "raw": "# by https://www.kaggle.com/dimartinot\ndef clean_inf_nan(df):\n    return df.replace([np.inf, -np.inf], np.nan)   \n\n# Cleaning infinite values to NaN\nX = clean_inf_nan(X)\n# X_test = clean_inf_nan(X_test )",
      "total-ns": 7275088716
    },
    {
      "raw": "gc.collect()",
      "total-ns": 70898396
    },
    {
      "raw": "# n_fold = 5\n# folds = TimeSeriesSplit(n_splits=n_fold)\n# folds = KFold(n_splits=5)",
      "total-ns": 215252
    },
    {
      "raw": "# params = {'num_leaves': 256,\n#           'min_child_samples': 79,\n#           'objective': 'binary',\n#           'max_depth': 13,\n#           'learning_rate': 0.03,\n#           \"boosting_type\": \"gbdt\",\n#           \"subsample_freq\": 3,\n#           \"subsample\": 0.9,\n#           \"bagging_seed\": 11,\n#           \"metric\": 'auc',\n#           \"verbosity\": -1,\n#           'reg_alpha': 0.3,\n#           'reg_lambda': 0.3,\n#           'colsample_bytree': 0.9,\n#           #'categorical_feature': cat_cols\n#          }\n# result_dict_lgb = train_model_classification(X=X, X_test=X_test, y=y, params=params, folds=folds, model_type='lgb', eval_metric='auc', plot_feature_importance=True,\n#                                                       verbose=500, early_stopping_rounds=200, n_estimators=5000, averaging='usual', n_jobs=-1)",
      "total-ns": 175749
    },
    {
      "raw": "# sub['isFraud'] = result_dict_lgb['prediction']\n# sub.to_csv('submission.csv', index=False)",
      "total-ns": 93817
    },
    {
      "raw": "# sub.head()",
      "total-ns": 124988
    },
    {
      "raw": "# pd.DataFrame(result_dict_lgb['oof']).to_csv('lgb_oof.csv', index=False)",
      "total-ns": 102284
    },
    {
      "raw": "# xgb_params = {'eta': 0.04,\n#               'max_depth': 5,\n#               'subsample': 0.85,\n#               'objective': 'binary:logistic',\n#               'eval_metric': 'auc',\n#               'silent': True,\n#               'nthread': -1,\n#               'tree_method': 'gpu_hist'}\n# result_dict_xgb = train_model_classification(X=X, X_test=X_test, y=y, params=xgb_params, folds=folds, model_type='xgb', eval_metric='auc', plot_feature_importance=False,\n#                                                       verbose=500, early_stopping_rounds=200, n_estimators=5000, averaging='rank')",
      "total-ns": 119466
    },
    {
      "raw": "# test = test.sort_values('TransactionDT')\n# test['prediction'] = result_dict_xgb['prediction']\n# sub['isFraud'] = pd.merge(sub, test, on='TransactionID')['prediction']\n# sub.to_csv('submission_xgb.csv', index=False)",
      "total-ns": 96844
    },
    {
      "raw": "# test = test.sort_values('TransactionDT')\n# test['prediction'] = result_dict_lgb['prediction'] + result_dict_xgb['prediction']\n# sub['isFraud'] = pd.merge(sub, test, on='TransactionID')['prediction']\n# sub.to_csv('blend.csv', index=False)",
      "total-ns": 98345
    }
  ]
}