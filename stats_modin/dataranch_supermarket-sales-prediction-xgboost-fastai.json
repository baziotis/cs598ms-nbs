{
  "cells": [
    {
      "raw": "import warnings\nwarnings.filterwarnings('ignore')",
      "total-ns": 3953197
    },
    {
      "raw": "# STEFANOS: Disable pip install\n# !pip install lazypredict\n# !pip install --upgrade pandas\n# !pip install fast-tabnet\n# !pip install fastai\n# !pip install pandas-profiling",
      "total-ns": 235556
    },
    {
      "raw": "#A program that takes a csv and trains models on it. Streamlined model selection.\n#==============================================================================\n\n# STEFANOS: Disable unneeded modules\n# #LazyPredict\n# import lazypredict\n# from lazypredict.Supervised import LazyRegressor\n# from lazypredict.Supervised import LazyClassifier\n# #Baysian Optimization\n# from bayes_opt import BayesianOptimization\n#Pandas stack\nimport os\n\nimport os\nos.environ[\"MODIN_ENGINE\"] = \"ray\"\nimport ray\nos.environ['MODIN_CPUS'] = \"4\"\nray.init(num_cpus=4, runtime_env={'env_vars': {'__MODIN_AUTOIMPORT_PANDAS__': '1'}})\nimport modin.pandas as pd\nimport pandas_profiling\nimport numpy as np\n# STEFANOS: Disable unneeded modules\n# #FastAI\nfrom fastai.tabular.all import *\nfrom fastai.tabular.core import *\n# #Plots\n# import matplotlib.pyplot as plt\n# import seaborn as sns\n#System\nimport os\nimport sys\nimport traceback\n#Fit an xgboost model\n# STEFANOS: Disable unneeded modules\n# from xgboost import XGBRegressor\n# from xgboost import XGBClassifier\n# from xgboost import plot_importance\n# from sklearn.metrics import mean_squared_error\n# from sklearn.metrics import roc_auc_score\n#Random\nimport random\n\n#TabNet\nfrom fast_tabnet.core import *\n\nimport shutil",
      "total-ns": 17724846123
    },
    {
      "raw": "# STEFANOS: Disable matplotlib inline\n# %matplotlib inline",
      "total-ns": 200828
    },
    {
      "raw": "# For Styling\n# STEFANOS: Disbale plotting\n# plt.style.use('seaborn-bright')",
      "total-ns": 100661
    },
    {
      "raw": "#Project Variables\n#===================================================================================================\nPROJECT_NAME = 'superstore'\nVARIABLE_FILES = False\n#Maximum amount of rows to take\n\n## -- STEFANOS -- Remove sample\n\n# SAMPLE_COUNT = 4000\nFASTAI_LEARNING_RATE = 1e-1\nAUTO_ADJUST_LEARNING_RATE = False\n#Set to True automatically infer if variables are categorical or continuous\nENABLE_BREAKPOINT = True\n#When trying to declare a column a continuous variable, if it fails, convert it to a categorical variable\nCONVERT_TO_CAT = False\nREGRESSOR = True\nSEP_DOLLAR = False\nSEP_COMMA = True\nSHUFFLE_DATA = True",
      "total-ns": 720702
    },
    {
      "raw": "input_dir = f'../input/{PROJECT_NAME}'\n# STEFANOS: Change the working path\n# param_dir = f'/kaggle/working/{PROJECT_NAME}'\nparam_dir = f'./working/{PROJECT_NAME}'\nTARGET = ''\nPARAM_DIR = param_dir\nprint(f'param_dir: {param_dir}')\nif not os.path.exists(param_dir):\n    os.makedirs(param_dir)\n#rename any file in param_dir/file that ends with csv to data.csv\nfor file in os.listdir(input_dir):\n    if file.endswith('.csv'):\n        print('CSV!')\n        if 'classification_results' not in file and 'regression_results' not in file:\n            #os.rename(f'{input_dir}/{file}', f'{param_dir}/data.csv')\n            shutil.copy(f'{input_dir}/{file}', f'{param_dir}/data.csv')\n        #os.rename(f'{param_dir}/{file}', f'{param_dir}/data.csv')\ntry:\n    # STEFANOS: Remove sample\n#     df = pd.read_csv(f'{param_dir}/data.csv', nrows=SAMPLE_COUNT)\n    df = pd.read_csv(f'{param_dir}/data.csv')\nexcept:\n    print(f'Please place a file named data.csv in {param_dir}')\n    #sys.exit()",
      "total-ns": 41494052
    },
    {
      "raw": "",
      "total-ns": 134787
    },
    {
      "raw": "df",
      "total-ns": 273079
    },
    {
      "raw": "factor = 10\ndf = pd.concat([df]*factor)\ndf.info()",
      "total-ns": 129079877
    },
    {
      "raw": "if SEP_DOLLAR:\n    #For every column in df, if the column contains a $, make a new column with the value without the $\n    for col in df.columns:\n        if '$' in df[col].to_string():\n            df[col + '_no_dollar'] = df[col].str.replace('$', '').str.replace(',', '')\n            #Try to convert this new column to a numeric type\n            try:\n                df[col + '_no_dollar'] = df[col + '_no_dollar'].apply(pd.to_numeric, errors='coerce').dropna()\n            except Exception:\n                print(f'{col} can not be converted to a float!')\n\n\nif SEP_COMMA:\n    #For every column in df, if the column contains a %, make a new column with the value without the %\n    for col in df.columns:\n        if '%' in df[col].to_string() or ',' in df[col].to_string():\n            df[col + '_processed'] = df[col].str.replace('%', '').str.replace(',', '')\n            #Try to convert this new column to a numeric type\n            try:\n                df[col + '_processed'] = df[col + '_processed'].apply(pd.to_numeric, errors='coerce').dropna()\n            except Exception:\n                print(f'{col} can not be converted to a float!')",
      "total-ns": 19649881436
    },
    {
      "raw": "",
      "total-ns": 130831
    },
    {
      "raw": "df",
      "total-ns": 289884
    },
    {
      "raw": "df.isna().sum()",
      "total-ns": 71818731
    },
    {
      "raw": "df.profile_report()",
      "total-ns": 7426457
    },
    {
      "raw": "# STEFANOS: Disable plotting\n# sns.heatmap(df.corr())\n_ = df.corr()",
      "total-ns": 15306954
    },
    {
      "raw": "# STEFANOS: Disable plotting\n# df.head().style.background_gradient(cmap = \"inferno\")\ndf.head()",
      "total-ns": 725800
    },
    {
      "raw": "# STEFANOS: Disable plotting\n# df.describe().T.style.background_gradient(cmap = \"viridis\")\ndf.describe().T",
      "total-ns": 26942305
    },
    {
      "raw": "df.columns",
      "total-ns": 281096
    },
    {
      "raw": "target = ''\ntarget_str = ''\n#The column closest to the end isPARAM_DIR the target variable that can be represented as a float is the target variable\ntargets = []\n#Loop through every possible target column (Continuous)\nfor i in range(len(df.columns)-1, 0, -1):\n    try:\n        df[df.columns[i]] = df[df.columns[i]].apply(pd.to_numeric, errors='coerce').dropna()\n        target = df.columns[i]\n        target_str = target.replace('/', '-')\n    except:\n        continue\n    print(f'Target Variable: {target}')\n    #Will be determined by the file name\n\n\n    #===================================================================================================\n\n    #Create project config files if they don't exist.\n    if not os.path.exists(param_dir):\n        #create param_dir\n        os.makedirs(PARAM_DIR)\n    if not os.path.exists(f'{PARAM_DIR}/cats.txt'):\n        #create param_dir\n        with open(f'{PARAM_DIR}/cats.txt', 'w') as f:\n            f.write('')\n    if not os.path.exists(f'{PARAM_DIR}/conts.txt'):\n        #create param_dir\n        with open(f'{PARAM_DIR}/conts.txt', 'w') as f:\n            f.write('')\n    if not os.path.exists(f'{PARAM_DIR}/cols_to_delete.txt'):\n        with open(f'{PARAM_DIR}/cols_to_delete.txt', 'w') as f:\n            f.write('')\n\n    df = df.drop_duplicates()\n    if SHUFFLE_DATA:\n        df = df.sample(frac=1).reset_index(drop=True)\n\n    # workaround for fastai/pytorch bug where bool is treated as object and thus erroring out.\n    for n in df:\n        if pd.api.types.is_bool_dtype(df[n]):\n            df[n] = df[n].astype('uint8')\n\n    with open(f'{PARAM_DIR}/cols_to_delete.txt', 'r') as f:\n        cols_to_delete = f.read().splitlines()\n    for col in cols_to_delete:\n        try:\n            del(df[col])\n        except:\n            pass\n    #try to fill in missing values now, otherwise FastAI will do it for us later\n    try:\n        df = df.fillna(0)\n    except:\n        pass\n    #print missing values\n    #print(df.isna().sum().sort_values(ascending=False))\n    #shrink df as much as possible\n    df = df_shrink(df)\n\n\n    #print types inside of df\n    #print(df.dtypes)\n\n\n    #Auto detect categorical and continuous variables\n    #==============================================================================\n    likely_cat = {}\n    for var in df.columns:\n        likely_cat[var] = 1.*df[var].nunique()/df[var].count() < 0.05 #or some other threshold\n\n    cats = [var for var in df.columns if likely_cat[var]]\n    conts = [var for var in df.columns if not likely_cat[var]]\n\n    #remove target from lists\n    try:\n        conts.remove(target)\n        cats.remove(target)\n    except:\n        pass\n    #Convert target to float\n    df[target] = df[target].apply(pd.to_numeric, errors='coerce').dropna()\n\n    print('CATS=====================')\n    print(cats)\n    print('CONTS=====================')\n    print(conts)\n\n    #Populate categorical and continuous lists\n    #==============================================================================\n\n    if VARIABLE_FILES == True:\n        with open(f'{PARAM_DIR}/cats.txt', 'r') as f:\n            cats = f.read().splitlines()\n\n        with open(f'{PARAM_DIR}/conts.txt', 'r') as f:\n            conts = f.read().splitlines()\n\n    #==============================================================================\n\n    #==============================================================================\n    procs = [Categorify, FillMissing, Normalize]\n    #print(df.describe().T)\n    # STEFANOS: Remove samples\n#     df = df[0:SAMPLE_COUNT]\n    splits = RandomSplitter()(range_of(df))\n\n    print((len(cats)) + len(conts))\n    #conts = []\n\n    #Convert cont variables to floats\n    #==============================================================================\n\n    #Convert cont variables to floats\n    #==============================================================================\n\n    for var in conts:\n        try:\n            df[var] = df[var].apply(pd.to_numeric, errors='coerce').dropna()\n        except:\n            print(f'Could not convert {var} to float.')\n            pass\n\n    #==============================================================================\n\n    #Experimental logic to add columns one-by-one to find a breakpoint\n    #==============================================================================\n    if ENABLE_BREAKPOINT == True:\n        temp_procs = [Categorify, FillMissing]\n        print('Looping through continuous variables to find breakpoint')\n        cont_list = []\n        for cont in conts:\n            focus_cont = cont\n            cont_list.append(cont)\n            #print(focus_cont)\n            try:\n                to = TabularPandas(df, procs=procs, cat_names=cats, cont_names=cont_list, y_names=target, y_block=RegressionBlock(), splits=splits)\n                del(to)\n            except:\n                print('Error with ', focus_cont)\n                #remove focus_cont from list\n                cont_list.remove(focus_cont)\n                #traceback.print_exc()\n                continue\n        #convert all continuous variables to floats\n        for var in cont_list:\n            try:\n                df[var] = df[var].apply(pd.to_numeric, errors='coerce').dropna()\n            except:\n                print(f'Could not convert {var} to float.')\n                cont_list.remove(var)\n                if CONVERT_TO_CAT == True:\n                    cats.append(var)\n                pass\n        print(f'Continuous variables that made the cut : {cont_list}')\n        print(f'Categorical variables that made the cut : {cats}')\n        #shrink df as much as possible\n        df = df_shrink(df)\n        #print(df.dtypes)\n\n    #==============================================================================\n\n    #Creating tabular object + quick preprocessing\n    #==============================================================================\n    to = None\n    if REGRESSOR == True:\n        try:\n            to = TabularPandas(df, procs, cats, conts, target, y_block=RegressionBlock(), splits=splits)\n        except:\n            conts = []\n            to = TabularPandas(df, procs, cats, conts, target, y_block=RegressionBlock(), splits=splits)\n    else:\n        try:\n            to = TabularPandas(df, procs, cats, conts, target, splits=splits)\n        except:\n            conts = []\n            to = TabularPandas(df, procs, cats, conts, target, splits=splits)\n\n### STEFANOS - Disabled ML code ###\n            \n#     #print(dir(to))\n#     #print(to.xs)\n#     dls = to.dataloaders()\n#     print(f'Tabular Object size: {len(to)}')\n#     try:\n#         dls.one_batch()\n#     except:\n#         print(f'problem with getting one batch of {PROJECT_NAME}')\n#     #==============================================================================\n\n#     #Extracting train and test sets from tabular object\n#     #==============================================================================\n\n#     X_train, y_train = to.train.xs, to.train.ys.values.ravel()\n#     X_test, y_test = to.valid.xs, to.valid.ys.values.ravel()\n\n#     #Make sure target isn't in independent columns\n#     if target in X_train and target in X_test:\n#         del(X_train[target])\n#         del(X_test[target])\n#     #create dataframe from X_train and y_train\n#     #export tabular object to csv\n#     pd.DataFrame(X_train).to_csv(f'{PARAM_DIR}/X_train_{target_str}.csv', index=False)\n#     pd.DataFrame(X_test).to_csv(f'{PARAM_DIR}/X_test_{target_str}.csv', index=False)\n#     pd.DataFrame(y_train).to_csv(f'{PARAM_DIR}/y_train_{target_str}.csv', index=False)\n#     pd.DataFrame(y_test).to_csv(f'{PARAM_DIR}/y_test_{target_str}.csv', index=False)\n\n#     #==============================================================================\n\n#     #==============================================================================\n\n#     #Ready for model selection!\n\n#     if REGRESSOR == True:\n#         try:\n#             reg = LazyRegressor(verbose=2, ignore_warnings=False, custom_metric=None)\n#             models, predictions = reg.fit(X_train, X_test, y_train, y_test)\n#             print(f'Project: {PROJECT_NAME}')\n#             print(PROJECT_NAME)\n#             print(f'Target: {target}')\n#             print(target)\n#             target_std = y_train.std()\n#             print(f'Target Standard Deviation: {target_std}')\n#             print(models)\n#             models['project'] = PROJECT_NAME\n#             models['target'] = target\n#             models['target_std'] = target_std\n#             #rename index of \n#             models.to_csv(f'{PARAM_DIR}/regression_results_{target_str}.csv', mode='a', header=True, index=True)\n#         except:\n#             print('Issue during lazypredict analysis')\n#     else:\n#         #TODO: remove this\n#         try:\n#             clf = LazyClassifier(verbose=2, ignore_warnings=False, custom_metric=None)\n#             models, predictions = clf.fit(X_train, X_test, y_train, y_test)\n#             print(f'Project: {PROJECT_NAME}')\n#             print(PROJECT_NAME)\n#             print(f'Target: {target}')\n#             print(target)\n#             print(f'Target Standard Deviation: {y_train.std()}')\n#             print(models)\n#             models.to_csv(f'{PARAM_DIR}/classification_results.csv', mode='a', header=False)\n#         except:\n#             print('Issue during lazypredict analysis')\n\n#     model_name = 'tabnet'\n\n#     # FastAI + pre-trained TabNet\n#     #==============================================================================\n#     learn = None\n#     i = 0\n#     while True:\n#         try:\n#             del learn\n#         except:\n#             pass\n#         try:\n#             learn = 0\n#             model = TabNetModel(get_emb_sz(to), len(to.cont_names), dls.c, n_d=64, n_a=64, n_steps=5, virtual_batch_size=256)\n#             # save the best model so far, determined by early stopping\n#             cbs = [SaveModelCallback(monitor='_rmse', comp=np.less, fname=f'{model_name}_{PROJECT_NAME}_{target_str}_best'), EarlyStoppingCallback()]\n#             learn = Learner(dls, model, loss_func=MSELossFlat(), metrics=rmse, cbs=cbs)\n#             #learn = get_learner(to)\n#             if(learn != 0):\n#                 break\n#             if i > 50:\n#                 break\n#         except:\n#             i += 1\n#             print('Error in FastAI TabNet')\n#             traceback.print_exc()\n#             continue\n#     try:\n#         #display learning rate finder results\n#         x = learn.lr_find()\n#     except:\n#         pass\n#     if AUTO_ADJUST_LEARNING_RATE == True:\n#         FASTAI_LEARNING_RATE = x.valley\n#     print(f'LEARNING RATE: {FASTAI_LEARNING_RATE}')\n#     try:\n#         if i < 50:\n#             learn.fit_one_cycle(20, FASTAI_LEARNING_RATE)\n#             plt.figure(figsize=(10, 10))\n#             try:\n#                 ax = learn.show_results()\n#                 plt.show(block=True)\n#             except:\n#                 print('Could not show results')\n#                 pass\n#     except:\n#         print('Could not fit model')\n#         traceback.print_exc()\n#         pass\n\n#     #==============================================================================\n\n#     #fit an xgboost model\n#     #==============================================================================\n#     if REGRESSOR == True:\n#         xgb = XGBRegressor()\n#     else:\n#         xgb = XGBClassifier()\n#     try:\n#         xgb = XGBRegressor()\n#         xgb.fit(X_train, y_train)\n#         y_pred = xgb.predict(X_test)\n#         print('XGBoost Predictions vs Actual==========')\n#         print(pd.DataFrame({'actual': y_test, 'predicted': y_pred}).head())\n#         print('XGBoost RMSE: ', np.sqrt(mean_squared_error(y_test, y_pred)))\n#         #save feature importance plot to file\n#         plot_importance(xgb)\n#         plt.title(f'XGBoost Feature Importance for {PROJECT_NAME} | Target : {target}', wrap=True)\n#         plt.tight_layout()\n#         plt.show()\n#         plt.savefig(f'{PARAM_DIR}/xgb_feature_importance_{target_str}.png')\n#         fi_df = pd.DataFrame([xgb.get_booster().get_score()]).T\n#         fi_df.columns = ['importance']\n#         #create a column based off the index called feature\n#         fi_df['feature'] = fi_df.index\n#         #create a dataframe of feature importance\n#         fi_df = fi_df[['feature', 'importance']]\n#         fi_df.to_csv(f'{PARAM_DIR}/xgb_feature_importance_{target_str}.csv', index=False)\n#         #xgb_fi = pd.DataFrame(xgb.feature_importances_, index=X_train.columns, columns=['importance'])\n#         #xgb_fi.to_csv(f'{PARAM_DIR}/xgb_feature_importance_{target_str}.csv')\n#         #print('XGBoost AUC: ', roc_auc_score(y_test, y_pred))\n#     except:\n#         traceback.print_exc()\n#         print('XGBoost failed')",
      "total-ns": 3151689497
    },
    {
      "raw": "# out_dir = f'./{PROJECT_NAME}'\n# xgb_feature_importance_csvs = []\n\n# for file in os.listdir(out_dir):\n#     if 'xgb_feature_importance' in file and '.csv' in file:\n#         xgb_feature_importance_csvs.append(pd.read_csv(os.path.join(out_dir, file)))\n\n# xgb_feature_importance = pd.concat(xgb_feature_importance_csvs,axis=0)\n# xgb_feature_importance.rename(columns={'Unnamed: 0': 'feature'}, inplace=True)\n# print(xgb_feature_importance.head())\n# xgb_feature_importance.groupby('feature')['importance'].mean().sort_values(ascending=False).plot(kind='bar', title='XGBoost Overall Feature Importance', figsize=(20, 10))",
      "total-ns": 231919
    },
    {
      "raw": "df.isna().sum()",
      "total-ns": 2394517
    }
  ]
}